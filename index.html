<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenhao Wang</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:64%;vertical-align:middle">
              <p style="text-align:left">
                <name>Wenhao Wang</name>
              </p>
              <p>I am a Research Assistant in <a href="http://research.baidu.com/">Baidu Research</a> co-supervised by <a href="https://yifansun-reid.github.io/">Yifan Sun</a> and <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>. My research interest is deep metric learning and computer vision.
                Prior to Baidu, I was a Remote Research Intern in <a href="https://www.inceptioniai.org/">Inception Institute of Artificial Intelligence (IIAI)</a> from 2020 to 2021,
                where I was supervised by <a href="https://zhaofang0627.github.io/">Fang Zhao</a> and
                <a href="https://shengcailiao.github.io/">Shengcai Liao</a>.
                I gained my bachelor degree from Beihang University in 2021 with Shenyuan Medal (Top 10 Undergraduate). I will begin my Ph.D. journey in <a href="http://reler.net/">ReLER</a> lab.
              </p>
              <p style="text-align:left">
                <a href="mailto:wangwenhao0716@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=k3mq3XMAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/WangWenhao0716/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/WenhaoWang20">Twitter</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:44%;max-width:44%;vertical-align:middle">
              <img style="width:78%;max-width:78%" alt="profile photo" src="pics/photo_yuan.jpg">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Papers</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/ASL.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2205.12358">
                <font color=#1772d0>  <papertitle>A Benchmark and Asymmetrical-Similarity Learning for Practical Image Copy Detection</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yifan Sun,
              Yi Yang
              <br>
              <em>Arxiv</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2205.12358">arXiv</a> /
              <a href="bibs/asl.bib">bibtex</a>
              <p></p>
              <p>We contribute a new ICD dataset, i.e., Negative-Distractor for Edited Copy (NDEC), with emphasis on the seldom-noticed hard negative problem. We propose a novel Asymmetric-Similarity Learning (ASL) method for ICD. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/awb.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9677903">
                <font color=#1772d0>  <papertitle>Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Fang Zhao,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>TIP</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2006.06525">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/Attentive-WaveBlock">Code</a> /
              <a href="bibs/awb.bib">bibtex</a>
              <p></p>
              <p>This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/anchor_udf.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Anchored_Unsigned_Distance_Functions_With_Gradient_Direction_Alignment_for_ICCV_2021_paper.pdf">
                <font color=#1772d0>  <papertitle>Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for
                  Single-view Garment Reconstruction</papertitle></font>
              </a>
              <br>
              Fang Zhao,
              <strong>Wenhao Wang</strong>,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>ICCV</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08478">arXiv</a> /
              <a href="https://github.com/zhaofang0627/AnchorUDF">Code</a> /
              <a href="bibs/anchor_udf.bib">bibtex</a>
              <p></p>
              <p>We propose a novel learnable Anchored Unsigned Distance Function (AnchorUDF)
                representation for 3D garment reconstruction from a single image.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/domainmix.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.11953.pdf">
                <font color=#1772d0>  <papertitle>DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Shengcai Liao,
              Fang Zhao,
              Cuicui Kang,
              Ling Shao
              <br>
              <em>BMVC</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2011.11953">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/DomainMix">Code</a> /
              <a href="bibs/domainmix.bib">bibtex</a>
              <p></p>
              <p>We propose a new person re-identification task, i.e. how to use labeled synthetic dataset and unlabeled real-world dataset to train
a universal model. A DomainMix framework is introduced to give a basic solution to the task.</p>
            </td>
          </tr>
          
          
          </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <br />
              <br />
              <br />
              <br />
                  <heading>Competitions</heading>
                              </td>
                            </tr>
                          </tbody></table>
                          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/ebay.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/fgvc9/competitions/ebay-eproduct-search">
                                  <font color=#1772d0>  <papertitle>FGVC9: eBay eProduct Visual Search Challenge</papertitle></font>
                                </a>
                                <br>
                                <em>CVPR</em>, 2022 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://www.google.com/url?q=https%3A%2F%2Fnam10.safelinks.protection.outlook.com%2F%3Furl%3Dhttps%253A%252F%252Feval.ai%252Fweb%252Fchallenges%252Fchallenge-page%252F1541%252Foverview%26data%3D04%257C01%257Cjiayuan%2540ebay.com%257C08a4dd0ae0d84ae461e508d9faf472e9%257C46326bff992841a0baca17c16c94ea99%257C0%257C0%257C637816751311135945%257CUnknown%257CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%253D%257C0%26sdata%3DyvHpU3%252FHDLgN62B9sOsRfAAjvAEhf7vPfFqT62khzfM%253D%26reserved%3D0&sa=D&sntz=1&usg=AOvVaw2H95-0UjmC8We3_skxss7_">Introduction</a> /
                                <a href="http://arxiv.org/abs/2207.12994">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/V2L">Code</a> /
                                <a href="https://github.com/WangWenhao0716/wangwenhao0716.github.io/blob/main/pics/1st-place-certificate-eproduct-fgvc9.pdf">Certificate</a>
                                <p></p>
                                <p>The paper demonstrates the effectiveness of vision-language models in product retrieval tasks for the first time.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/isc_1.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/isc2021">
                                  <font color=#1772d0>  <papertitle>Facebook AI Image Similarity Challenge: Matching Track</papertitle></font>
                                </a>
                                <br>
                                <em>NeurIPS</em>, 2021 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://www.youtube.com/watch?v=eOXd0sYpRW0">Introduction</a> /
                                <a href="https://arxiv.org/abs/2111.07090">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/ISC-Track1-Submission">Code</a> /
                                <a href="https://drive.google.com/file/d/1ayk8fStGTZVf4odO1T1quDLAdIkCfccI/view?usp=sharing">Presentation</a>
                                <p></p>
                                <p>In this paper, a data-driven and local-verification approach is proposed. </p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/isc_2.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/isc2021">
                                  <font color=#1772d0>  <papertitle>Facebook AI Image Similarity Challenge: Descriptor Track</papertitle></font>
                                </a>
                                <br>
                                <em>NeurIPS</em>, 2021 <font color="red"><strong>(Rank 3)</strong></font>
                                <br>
                                <a href="https://www.youtube.com/watch?v=eOXd0sYpRW0">Introduction</a> /
                                <a href="https://arxiv.org/abs/2111.08004">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/ISC-Track2-Submission">Code</a> /
                                <a href="https://drive.google.com/file/d/1ayk8fStGTZVf4odO1T1quDLAdIkCfccI/view?usp=sharing">Presentation</a>
                                <p></p>
                                <p>In this paper, a bag of tricks and a strong baseline are proposed for image copy detection.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/vos.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://youtube-vos.org/challenge/2021/">
                                  <font color=#1772d0>  <papertitle>The 3rd Large-scale Video Object Segmentation Challenge: Video Object Segmentation Track</papertitle></font>
                                </a>
                                <br>
                                <em>CVPR</em>, 2021 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://youtube-vos.org/">Introduction</a> /
                                <a href="https://youtube-vos.org/assets/challenge/2021/reports/VOS_1_Yang.pdf">Solution</a> /
                                <a href="https://github.com/z-x-yang/AOT">Code</a> /
                                <a href="https://github.com/WangWenhao0716/wangwenhao0716.github.io/blob/main/pics/Certificate_vos1.pdf">Certificate</a>
                                <p></p>
                                <p>This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object
                  scenarios.</p>
                              </td>
                            </tr>
                            
          
                            
          </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Professional Activities</heading>
                            <tr>
                              <td style="padding:20px;width:100%;vertical-align:middle">
                                <p><strong>Journal Reviewer</strong> of Transaction on Image Processing, Transactions on Circuits and Systems for Video Technology, Knowledge-Based Systems, IEEE Intelligent Transportation Systems Transactions, IEEE/CAA Journal of Automatica Sinica, and Journal of Visual Communication and Image Representation.</p>
                                <p><strong>Conference Reviewer</strong> of AAAI, ICIP.</p>
                              </td>
                            </tr>
              
            </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Invited Talks</heading>
                            <tr>
                              <td style="padding:20px;width:100%;vertical-align:middle">
                                <p>[2020~2022] Series Talk: "Artificial Intelligence on GPU with High Performance Computing Platform" at Beihang University.</p>
                                <p>[2021.6.10] "Domain-invariant Feature Learning: Research on Generalizable Person Re-identification" at Beihang University.</p>
                                <p>[2021.12.16] "Bag of Tricks and a Strong Baseline for Image Copy Detection" at Beihang University.</p>
                                <p>[2021.12.23] "D2LV: A Data-Driven and Local-Verification Approach for Image Copy Detection" at Beihang University.</p>
                                <p>[2021.12.30] "Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond" at Beihang University.</p>
                                <p>[2022.1.6] "DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations" at Beihang University.</p>
                              </td>
                            </tr>
                            
          



        </tbody></table>
      </td>
    </tr>

  </table>
</body>

</html>
