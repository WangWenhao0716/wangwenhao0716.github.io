<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenhao Wang</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:64%;vertical-align:middle">
              <p style="text-align:left">
                <name>Wenhao Wang</name>
              </p>
              <p>I am a Ph.D. student in <a href="http://reler.net/">ReLER</a>, <a href="https://www.uts.edu.au/research/australian-artificial-intelligence-institute">AAII</a>, <a href="https://www.uts.edu.au">University of Technology Sydney</a>, supervised by <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>. My research interest is visual copy detection, deep metric learning and computer vision.
                Before ReLER, I was a Research Assistant in <a href="http://research.baidu.com/">Baidu Research</a> co-supervised by <a href="https://yifansun-reid.github.io/">Yifan Sun</a> and <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ&hl=zh-CN">Yi Yang</a>.
                Prior to Baidu, I was a Remote Research Intern in <a href="https://www.inceptioniai.org/">Inception Institute of Artificial Intelligence (IIAI)</a> from 2020 to 2021,
                where I was supervised by <a href="https://zhaofang0627.github.io/">Fang Zhao</a> and
                <a href="https://shengcailiao.github.io/">Shengcai Liao</a>.
                I gained my bachelor degree from Beihang University in 2021 with Shenyuan Medal (Top 10 Undergraduate). 
              </p>
              <p style="text-align:left">
                <a href="mailto:wangwenhao0716@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=k3mq3XMAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href=" https://openreview.net/profile?id=~Wenhao_Wang2">OpenReview</a> &nbsp/&nbsp
                <a href="https://github.com/WangWenhao0716/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/WenhaoWang20">Twitter</a> 
                
              </p>
            </td>
            <td style="padding:2.5%;width:44%;max-width:44%;vertical-align:middle">
              <img style="width:78%;max-width:78%" alt="profile photo" src="pics/photo_yuan.jpg">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Research Papers</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/AnyPattern.jpg' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://anypattern.github.io/">
                <font color=#1772d0>  <papertitle>AnyPattern: Towards In-context Image Copy Detection</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yifan Sun,
              Zhentao Tan,
              Yi Yang
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2404.13788">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/AnyPattern">Code (ICD)</a> /
              <a href="https://github.com/WangWenhao0716/AnyPatternStyle">Code (Style)</a> /
              <a href="https://huggingface.co/datasets/WenhaoWang/AnyPattern">Data</a>  /
              <a href="bibs/anypattern.bib">bibtex</a> 
              <p></p>
              <p> This paper explores in-context learning for image copy detection (ICD), i.e., prompting an ICD model to identify replicated images with new tampering patterns without the need for additional training. To accommodate the “seen → unseen” generalization scenario, we construct the first large-scale pattern dataset named AnyPattern, which has the largest number of tamper patterns (90 for training and 10 for testing) among all the existing ones. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/PEICD.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://wangwenhao0716.github.io/">
                <font color=#1772d0>  <papertitle>Pattern-Expandable Image Copy Detection</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yifan Sun,
              Yi Yang
              <br>
              <em>IJCV</em>, 2024
              <br>
              <a href="https://github.com/WangWenhao0716/PEICD">Code</a> /
              <a href="https://huggingface.co/datasets/WenhaoWang/PE-ICD">Data</a>  /
              <a href="bibs/peicd.bib">bibtex</a> 
              <p></p>
              <p> This paper proposes a specific open-world visual recognition task, i.e. Pattern-Expandable Image Copy Detection (PE-ICD). To lay the foundation for PE-ICD research, we propose Pattern Stripping (P-Strip), which separates the tamper patterns by decomposing a query into a content feature and multiple pattern features. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/vidprom.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://vidprom.github.io/">
                <font color=#1772d0>  <papertitle>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yi Yang
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.06098">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/VidProM">Github</a> /
              <a href="https://huggingface.co/datasets/WenhaoWang/VidProM">Hugging Face</a>  /
              <a href="https://wisemodel.cn/datasets/WenhaoWang/VidProM">Wisemodel</a>  /
              <a href="bibs/vidprom.bib">bibtex</a> /
              <a href="https://zhuanlan.zhihu.com/p/686262640">Zhihu</a> 
              <p>✨ <em><a href="https://web.archive.org/web/20240319061213/https://huggingface.co/datasets"><strong>Top6</strong>/121,084</a> in the Hugging Face Dataset Trending List on Mar. 19th 2024.</p></em>
              <p></p>
              <p> VidProM is the first dataset featuring 1.67 million unique text-to-video prompts and 6.69 million videos generated from 4 different state-of-the-art diffusion models. It inspires many exciting new research areas, such as Text-to-Video Prompt Engineering, Efficient Video Generation, Fake Video Detection, and Video Copy Detection for Diffusion Models.  </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/TransHP.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.06385">
                <font color=#1772d0>  <papertitle>TransHP: Image Classification with Hierarchical Prompting</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yifan Sun,
              Wei Li,
              Yi Yang
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2304.06385">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/TransHP">Code</a> /
              <a href="bibs/transhp.bib">bibtex</a> /
              <a href="pics/NeurIPS2023_poster_1460.pdf">poster</a> 
              <p></p>
              <p>This paper explores a hierarchical prompting mechanism for the hierarchical image classification (HIC) task. Different from prior HIC methods, our hierarchical prompting is the first to explicitly inject ancestor-class information as a tokenized hint that benefits the descendant-class discrimination.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/ASL.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2205.12358">
                <font color=#1772d0>  <papertitle>A Benchmark and Asymmetrical-Similarity Learning for Practical Image Copy Detection</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Yifan Sun,
              Yi Yang
              <br>
              <em>AAAI</em>, 2023 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2205.12358">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/ASL">Dataset&Code</a> /
              <a href="bibs/asl.bib">bibtex</a> /
              <a href="pics/AAAI2023_poster_732.pdf">poster</a>
              <p></p>
              <p>We contribute a new ICD dataset, i.e., Negative-Distractor for Edited Copy (NDEC), with emphasis on the seldom-noticed hard negative problem. We propose a novel Asymmetric-Similarity Learning (ASL) method for ICD. </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/awb.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9677903">
                <font color=#1772d0>  <papertitle>Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Fang Zhao,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>TIP</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2006.06525">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/Attentive-WaveBlock">Code</a> /
              <a href="bibs/awb.bib">bibtex</a>
              <p></p>
              <p>This paper proposes a novel light-weight module, the Attentive WaveBlock (AWB), which can be integrated into the dual networks of mutual learning to enhance the complementarity.</p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/anchor_udf.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Learning_Anchored_Unsigned_Distance_Functions_With_Gradient_Direction_Alignment_for_ICCV_2021_paper.pdf">
                <font color=#1772d0>  <papertitle>Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for
                  Single-view Garment Reconstruction</papertitle></font>
              </a>
              <br>
              Fang Zhao,
              <strong>Wenhao Wang</strong>,
              Shengcai Liao,
              Ling Shao
              <br>
              <em>ICCV</em>, 2021 <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08478">arXiv</a> /
              <a href="https://github.com/zhaofang0627/AnchorUDF">Code</a> /
              <a href="bibs/anchor_udf.bib">bibtex</a>
              <p></p>
              <p>We propose a novel learnable Anchored Unsigned Distance Function (AnchorUDF)
                representation for 3D garment reconstruction from a single image.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='pics/domainmix.png' width=200; height="auto">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.11953.pdf">
                <font color=#1772d0>  <papertitle>DomainMix: Learning Generalizable Person Re-Identification Without Human Annotations</papertitle></font>
              </a>
              <br>
              <strong>Wenhao Wang</strong>,
              Shengcai Liao,
              Fang Zhao,
              Cuicui Kang,
              Ling Shao
              <br>
              <em>BMVC</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2011.11953">arXiv</a> /
              <a href="https://github.com/WangWenhao0716/DomainMix">Code</a> /
              <a href="bibs/domainmix.bib">bibtex</a>
              <p></p>
              <p>We propose a new person re-identification task, i.e. how to use labeled synthetic dataset and unlabeled real-world dataset to train
a universal model. A DomainMix framework is introduced to give a basic solution to the task.</p>
            </td>
          </tr>
          
          
          </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <br />
              <br />
              <br />
              <br />
                  <heading>Competitions</heading>
                              </td>
                            </tr>
                          </tbody></table>
                          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/vsc_des.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://www.drivendata.org/competitions/group/meta-video-similarity">
                                  <font color=#1772d0>  <papertitle>Meta AI Video Similarity Challenge: Descriptor Track</papertitle></font>
                                </a>
                                <br>
                                <strong>Wenhao Wang</strong>,
                                Yifan Sun,
                                Yi Yang
                                <br>
                                <em>CVPR</em>, 2023 <font color="red"><strong>(Rank 2)</strong></font>
                                <br>
                                <a href="https://www.drivendata.org/competitions/101/meta-video-similarity-descriptor/">Introduction</a> /
                                <a href="https://arxiv.org/abs/2304.10305">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/VSC-DescriptorTrack-Submission">Code</a> /
                                <a href="pics/FCPL.pdf">Presentation</a>
                                <p></p>
                                <p>We propose Feature-Compatible Progressive Learning (FCPL), which trains various models that produce mutually-compatible features.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/vsc_mat.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://www.drivendata.org/competitions/group/meta-video-similarity">
                                  <font color=#1772d0>  <papertitle>Meta AI Video Similarity Challenge: Matching Track</papertitle></font>
                                </a>
                                <br>
                                <strong>Wenhao Wang</strong>,
                                Yifan Sun,
                                Yi Yang
                                <br>
                                <em>CVPR</em>, 2023 <font color="red"><strong>(Rank 2)</strong></font>
                                <br>
                                <a href="https://www.drivendata.org/competitions/106/meta-video-similarity-matching/">Introduction</a> /
                                <a href="https://arxiv.org/abs/2304.10305">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/VSC-MatchingTrack-Submission">Code</a> /
                                <a href="pics/FCPL.pdf">Presentation</a>
                                <p></p>
                                <p>We use Temporal Network (TN) to ensemble the features from the descriptor track directly.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/ebay.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/fgvc9/competitions/ebay-eproduct-search">
                                  <font color=#1772d0>  <papertitle>FGVC9: eBay eProduct Visual Search Challenge</papertitle></font>
                                </a>
                                <br>
                                <strong>Wenhao Wang</strong>,
                                Yifan Sun,
                                Zongxin Yang,
                                Yi Yang
                                <br>
                                <em>CVPR</em>, 2022 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://www.google.com/url?q=https%3A%2F%2Fnam10.safelinks.protection.outlook.com%2F%3Furl%3Dhttps%253A%252F%252Feval.ai%252Fweb%252Fchallenges%252Fchallenge-page%252F1541%252Foverview%26data%3D04%257C01%257Cjiayuan%2540ebay.com%257C08a4dd0ae0d84ae461e508d9faf472e9%257C46326bff992841a0baca17c16c94ea99%257C0%257C0%257C637816751311135945%257CUnknown%257CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%253D%257C0%26sdata%3DyvHpU3%252FHDLgN62B9sOsRfAAjvAEhf7vPfFqT62khzfM%253D%26reserved%3D0&sa=D&sntz=1&usg=AOvVaw2H95-0UjmC8We3_skxss7_">Introduction</a> /
                                <a href="http://arxiv.org/abs/2207.12994">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/V2L">Code</a> /
                                <a href="pics/1st-place-certificate-eproduct-fgvc9.pdf">Certificate</a>
                                <p></p>
                                <p>The paper demonstrates the effectiveness of vision-language models in product retrieval tasks for the first time.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/isc_1.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/isc2021">
                                  <font color=#1772d0>  <papertitle>Facebook AI Image Similarity Challenge: Matching Track</papertitle></font>
                                </a>
                                <br>
                                <strong>Wenhao Wang</strong>,
                                Yifan Sun,
                                Weipu Zhang,
                                Yi Yang
                                <br>
                                <em>NeurIPS</em>, 2021 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://www.youtube.com/watch?v=eOXd0sYpRW0">Introduction</a> /
                                <a href="https://arxiv.org/abs/2111.07090">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/ISC-Track1-Submission">Code</a> /
                                <a href="pics/neurips_pre.pdf">Presentation</a>
                                <p></p>
                                <p>In this paper, a data-driven and local-verification approach is proposed. </p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/isc_2.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://sites.google.com/view/isc2021">
                                  <font color=#1772d0>  <papertitle>Facebook AI Image Similarity Challenge: Descriptor Track</papertitle></font>
                                </a>
                                <br>
                                <strong>Wenhao Wang</strong>,
                                Yifan Sun,
                                Weipu Zhang,
                                Yi Yang
                                <br>
                                <em>NeurIPS</em>, 2021 <font color="red"><strong>(Rank 3)</strong></font>
                                <br>
                                <a href="https://www.youtube.com/watch?v=eOXd0sYpRW0">Introduction</a> /
                                <a href="https://arxiv.org/abs/2111.08004">Solution</a> /
                                <a href="https://github.com/WangWenhao0716/ISC-Track2-Submission">Code</a> /
                                <a href="pics/neurips_pre.pdf">Presentation</a>
                                <p></p>
                                <p>In this paper, a bag of tricks and a strong baseline are proposed for image copy detection.</p>
                              </td>
                            </tr>
                            
                            <tr>
                              <td style="padding:20px;width:25%;vertical-align:middle">
                                <div class="one">
                                  <img src='pics/vos.png' width=200; height="auto">
                                </div>
                              </td>
                              <td style="padding:20px;width:75%;vertical-align:middle">
                                <a href="https://youtube-vos.org/challenge/2021/">
                                  <font color=#1772d0>  <papertitle>The 3rd Large-scale Video Object Segmentation Challenge: Video Object Segmentation Track</papertitle></font>
                                </a>
                                <br>
                                Zongxin Yang,
                                Jian Zhang,
                                <strong>Wenhao Wang</strong>,
                                etc
                                <br>
                                <em>CVPR</em>, 2021 <font color="red"><strong>(Rank 1)</strong></font>
                                <br>
                                <a href="https://youtube-vos.org/">Introduction</a> /
                                <a href="https://youtube-vos.org/assets/challenge/2021/reports/VOS_1_Yang.pdf">Solution</a> /
                                <a href="https://github.com/z-x-yang/AOT">Code</a> /
                                <a href="pics/Certificate_vos1.pdf">Certificate</a>
                                <p></p>
                                <p>This paper investigates how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object
                  scenarios.</p>
                              </td>
                            </tr>
                            
          
                            
          </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Professional Activities</heading>
                            <tr>
                              <td style="padding:20px;width:100%;vertical-align:middle">
                                <p><strong>Journal Reviewer</strong> of Transactions on Pattern Analysis and Machine Intelligence, International Journal of Computer Vision, Transactions on Image Processing, Transactions on Circuits and Systems for Video Technology, Knowledge-Based Systems, Transactions on Intelligent Transportation Systems, IEEE/CAA Journal of Automatica Sinica, 
Transactions on Big Data, Transactions on Artificial Intelligence, Journal of Visual Communication and Image Representation, and Neural Networks.</p>
                                <p><strong>Conference Reviewer</strong> of ICLR, ICML, NeurIPS, CVPR, ICCV, ECCV, AAAI, and ACM MM.</p>
                              </td>
                            </tr>
              
           
                            
          



        </tbody></table>
      </td>
    </tr>

  </table>
</body>

</html>
